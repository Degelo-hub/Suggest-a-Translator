{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693c2b81-b729-448a-a157-6b4cb9a99a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "51e37ca0-1030-496a-bc10-afca2bd21b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08a0eae1620d46a5a848122f5ebec749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), accept='.pdf,.docx,.txt,.xlsx,.xls', description='Upload')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# upload file (.pdf,.docx,.txt,.xlsx)\n",
    "import os\n",
    "import fitz  # PyMuPDF (für PDF)\n",
    "import chardet\n",
    "from docx import Document\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "import pandas as pd\n",
    "\n",
    "def excel_to_txt(file_content, filename):\n",
    "    df = pd.read_excel(file_content, engine='openpyxl')\n",
    "    output_filename = filename.replace('.xlsx', '.txt').replace('.xls', '.txt')\n",
    "    df.to_csv(output_filename, sep='\\t', index=False)\n",
    "    return output_filename\n",
    "\n",
    "upload = widgets.FileUpload(accept='.pdf,.docx,.txt,.xlsx,.xls', multiple=False)\n",
    "display(upload)\n",
    "\n",
    "def on_upload_change(change):\n",
    "    for filename, file_info in upload.value.items():\n",
    "        file_content = file_info['content']\n",
    "        if filename.lower().endswith('.xlsx') or filename.lower().endswith('.xls'):\n",
    "            with open('temp.xlsx', 'wb') as f:\n",
    "                f.write(file_content)\n",
    "            output_filename = excel_to_txt('temp.xlsx', filename)\n",
    "            print(f\"Excel-Datei wurde in {output_filename} umgewandelt!\")\n",
    "        elif filename.lower().endswith('.docx'):\n",
    "            # DOCX-Logik hier einfügen\n",
    "            pass\n",
    "        elif filename.lower().endswith('.pdf'):\n",
    "            # PDF-Logik hier einfügen\n",
    "            pass\n",
    "        elif filename.lower().endswith('.txt'):\n",
    "            # TXT-Logik hier einfügen\n",
    "            pass\n",
    "\n",
    "upload.observe(on_upload_change, names='value')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "82efc159-4e56-4cbe-bd89-a4315335688d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Verwendete Textdatei: 162853_CP Liban_FR.txt\n"
     ]
    }
   ],
   "source": [
    "# Upload-Daten korrekt auslesen (auch in neueren Jupyter-Versionen)\n",
    "if isinstance(upload.value, dict):  # alte Struktur\n",
    "    uploaded_file = next(iter(upload.value.items()))\n",
    "    uploaded_filename = uploaded_file[0]\n",
    "    uploaded_bytes = uploaded_file[1][\"content\"]\n",
    "elif isinstance(upload.value, tuple) and len(upload.value) > 0:  # neue Struktur\n",
    "    uploaded_file = upload.value[0]\n",
    "    uploaded_filename = uploaded_file[\"name\"]\n",
    "    uploaded_bytes = uploaded_file[\"content\"]\n",
    "else:\n",
    "    raise ValueError(\"⚠️ Keine Datei hochgeladen.\")\n",
    "\n",
    "# Datei speichern\n",
    "with open(uploaded_filename, \"wb\") as f:\n",
    "    f.write(uploaded_bytes)\n",
    "def convert_to_txt(file_path):\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "    if ext == \".txt\":\n",
    "        return file_path\n",
    "    elif ext == \".pdf\":\n",
    "        doc = fitz.open(file_path)\n",
    "        text = \"\\n\".join([page.get_text() for page in doc])\n",
    "    elif ext == \".docx\":\n",
    "        doc = Document(file_path)\n",
    "        text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "    elif ext in (\".xlsx\", \".xls\"):\n",
    "        # Excel-Datei einlesen\n",
    "        df = pd.read_excel(file_path, engine='openpyxl')\n",
    "        # Als TXT (Tab-getrennt) speichern\n",
    "        txt_path = os.path.splitext(file_path)[0] + \".txt\"\n",
    "        df.to_csv(txt_path, sep='\\t', index=False, encoding='utf-8')\n",
    "        return txt_path\n",
    "    else:\n",
    "        raise ValueError(\"❌ Nur PDF, DOCX, TXT oder XLS/XLSX erlaubt.\")\n",
    "\n",
    "    txt_path = os.path.splitext(file_path)[0] + \".txt\"\n",
    "    with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "    return txt_path\n",
    "\n",
    "# Hauptvariable setzen\n",
    "DOCUMENT_NAME = convert_to_txt(uploaded_filename)\n",
    "print(f\"📄 Verwendete Textdatei: {DOCUMENT_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "096bcc09-20d3-4bc8-a3a9-0baa268d22a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sachgebiete aus CSV geladen (31 Sachgebiete)\n",
      "\n",
      "🔎 Erkanntes Encoding: utf-8\n",
      "✅ Textdatei '162853_CP Liban_FR.txt' erfolgreich eingelesen (2616 Zeichen)\n",
      "\n",
      "✅ Termini-Zählung abgeschlossen.\n",
      "\n",
      "📊 Gefundene Termini je Sachgebiet:\n",
      "\n",
      "🗂️  Diplomatie und Protokoll:\n",
      "   - communiqué de presse: 1\n",
      "\n",
      "🗂️  HR:\n",
      "   - spécialiste: 1\n",
      "\n",
      "🗂️  Struktur und Institution:\n",
      "   - Direction du développement et de la coopération: 1\n",
      "   - domaine: 1\n",
      "   - Aide humanitaire: 1\n",
      "   - direction: 1\n",
      "\n",
      "🗂️  Entwicklung und Zusammenarbeit:\n",
      "   - Direction du développement et de la coopération: 1\n",
      "   - direction: 1\n",
      "\n",
      "🗂️  Humanitäre Hilfe:\n",
      "   - Aide humanitaire: 1\n",
      "   - aide humanitaire: 1\n",
      "\n",
      "🗂️  Geschäftsverwaltung:\n",
      "   - destruction: 1\n",
      "\n",
      "🗂️  Buchhaltung:\n",
      "   - projet: 1\n",
      "\n",
      "🏅 Top 3 Sachgebiete: ['Struktur und Institution', 'Entwicklung und Zusammenarbeit', 'Humanitäre Hilfe']\n",
      "\n",
      "✅ CSV-Datei '162853_CP Liban_FR_top3.csv' wurde erfolgreich erstellt.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Word Counter\n",
    "\n",
    "import chardet\n",
    "import re\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import os  # <-- NEW: for filename handling\n",
    "\n",
    "# === Konfigurierbarer Dateiname ===\n",
    "#DOCUMENT_NAME = \"162514_Focus Afrika_EN.txt\"  # <--- Nur hier anpassen!\n",
    "\n",
    "# === Schritt 1: Sachgebiet-CSV einlesen ===\n",
    "sachgebiete = defaultdict(list)\n",
    "\n",
    "with open(\"ED_Pub_Sachgebiet_multilingual5.csv\", \"r\", encoding=\"utf-8\") as file:\n",
    "    reader = csv.DictReader(file, delimiter=\";\")\n",
    "    for row in reader:\n",
    "        sachgebiet_roh = row[\"Header.Sachgebiete\"]\n",
    "        sachgebiet_liste = [s.strip() for s in sachgebiet_roh.split(\";\")]\n",
    "        for sachgebiet in sachgebiet_liste:\n",
    "            for key in row.keys():\n",
    "                if key.startswith((\"DES\", \"ENG\", \"FRS\", \"ITS\")) and row[key]:\n",
    "                    sachgebiete[sachgebiet].append(row[key])\n",
    "\n",
    "print(f\"✅ Sachgebiete aus CSV geladen ({len(sachgebiete)} Sachgebiete)\\n\")\n",
    "\n",
    "# === Schritt 2: Encoding automatisch erkennen ===\n",
    "def detect_encoding(dateipfad):\n",
    "    with open(dateipfad, \"rb\") as f:\n",
    "        rawdata = f.read(10000)\n",
    "        result = chardet.detect(rawdata)\n",
    "        return result[\"encoding\"]\n",
    "\n",
    "# === Schritt 3: Neuen Text einlesen ===\n",
    "dateipfad = DOCUMENT_NAME\n",
    "encoding = detect_encoding(dateipfad)\n",
    "print(f\"🔎 Erkanntes Encoding: {encoding}\")\n",
    "\n",
    "with open(dateipfad, \"r\", encoding=encoding) as file:\n",
    "    neuer_text = file.read()\n",
    "\n",
    "print(f\"✅ Textdatei '{dateipfad}' erfolgreich eingelesen ({len(neuer_text)} Zeichen)\\n\")\n",
    "\n",
    "# === Schritt 4: Termini zählen\n",
    "sachgebiet_counts = defaultdict(int)\n",
    "gefundene_terme = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for sachgebiet, terme in sachgebiete.items():\n",
    "    for term in set(terme):\n",
    "        pattern = rf\"(?<!\\w){re.escape(term)}(?!\\w)\"\n",
    "        matches = re.findall(pattern, neuer_text, re.IGNORECASE)\n",
    "        count = len(matches)\n",
    "\n",
    "        if count > 0:\n",
    "            sachgebiet_counts[sachgebiet] += count\n",
    "            gefundene_terme[sachgebiet][term] += count\n",
    "\n",
    "print(\"✅ Termini-Zählung abgeschlossen.\\n\")\n",
    "\n",
    "# === Schritt 5: Ergebnisse anzeigen\n",
    "print(\"📊 Gefundene Termini je Sachgebiet:\\n\")\n",
    "\n",
    "for sachgebiet, terme in gefundene_terme.items():\n",
    "    print(f\"🗂️  {sachgebiet}:\")\n",
    "    for term, count in terme.items():\n",
    "        print(f\"   - {term}: {count}\")\n",
    "    print()\n",
    "\n",
    "# === Schritt 6: Top 3 Sachgebiete bestimmen\n",
    "top_sachgebiete = sorted(sachgebiet_counts.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "top_sachgebiete_namen = [sg for sg, _ in top_sachgebiete]\n",
    "\n",
    "while len(top_sachgebiete_namen) < 3:\n",
    "    top_sachgebiete_namen.append(\"\")\n",
    "\n",
    "print(f\"🏅 Top 3 Sachgebiete: {top_sachgebiete_namen}\\n\")\n",
    "\n",
    "# === Schritt 7: Ergebnis in eine CSV schreiben\n",
    "# Automatisch den Output-Dateinamen erzeugen\n",
    "basename, _ = os.path.splitext(DOCUMENT_NAME)\n",
    "csv_dateiname = f\"{basename}_top3.csv\"\n",
    "\n",
    "with open(csv_dateiname, mode=\"w\", encoding=\"utf-8\", newline=\"\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow([\"Dateiname\", \"Text\", \"Top-Sachgebiet 1\", \"Top-Sachgebiet 2\", \"Top-Sachgebiet 3\"])\n",
    "    writer.writerow([dateipfad, neuer_text] + top_sachgebiete_namen)\n",
    "\n",
    "print(f\"✅ CSV-Datei '{csv_dateiname}' wurde erfolgreich erstellt.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "e7aec00b-c148-4302-80f7-05d7e5c24b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Chunking\n",
    "# 1. CSV einlesen und vorbereiten\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Lade den BERT-Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# Lade das neue CSV\n",
    "df_new_text = pd.read_csv(csv_dateiname)\n",
    "\n",
    "# Spalten für das Chunking beibehalten\n",
    "metadata_cols = [\"Dateiname\", \"Top-Sachgebiet 1\", \"Top-Sachgebiet 2\", \"Top-Sachgebiet 3\"]\n",
    "\n",
    "# Store all chunked rows\n",
    "expanded_rows = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "8db61d76-d195-4597-8141-af1a40559d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (961 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Chunked CSV '162853_CP Liban_FR_top3_chunks.csv' wurde erfolgreich erstellt.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Chunking der Texte\n",
    "# Neuen Basename aus der CSV erzeugen\n",
    "basename, _ = os.path.splitext(csv_dateiname)\n",
    "chunks_csv_name = f\"{basename}_chunks.csv\"\n",
    "                                  \n",
    "# Hilfsfunktion zum Chunken von Texten\n",
    "def chunk_text(text, max_len=512):\n",
    "    # Zerlege den Text in Token\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    # Überprüfe, ob der Text zu lang ist, und chunk ihn dann in 512-Tokens\n",
    "    for i in range(0, len(tokens), max_len):\n",
    "        chunk_tokens = tokens[i:i + max_len]\n",
    "        chunk_text = tokenizer.convert_tokens_to_string(chunk_tokens)\n",
    "        yield chunk_text\n",
    "\n",
    "# Iteriere durch jede Zeile im DataFrame\n",
    "for idx, row in df_new_text.iterrows():\n",
    "    text = row[\"Text\"]\n",
    "    \n",
    "    # Chunken des Textes in Segmente von max. 512 Tokens\n",
    "    for chunk_id, chunk_text in enumerate(chunk_text(text), start=1):\n",
    "        new_row = {col: row[col] for col in metadata_cols}\n",
    "        new_row[\"Text\"] = chunk_text\n",
    "        new_row[\"original_row_id\"] = idx\n",
    "        new_row[\"chunk_id\"] = chunk_id\n",
    "        expanded_rows.append(new_row)\n",
    "\n",
    "# Neue DataFrame erstellen\n",
    "expanded_df = pd.DataFrame(expanded_rows)\n",
    "\n",
    "# Optional: Speichern\n",
    "\n",
    "expanded_df.to_csv(chunks_csv_name, index=False, encoding=\"utf-8\")\n",
    "print(f\"✅ Chunked CSV '{chunks_csv_name}' wurde erfolgreich erstellt.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "188f245f-0241-4b34-9a28-08d33fbb0ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done CSV '162853_CP Liban_FR_top3_chunks_done.csv' wurde erfolgreich erstellt.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Hinzufügen der \"Top-3 alle\"-Spalte\n",
    "# Neuen Basename aus der CSV erzeugen\n",
    "basename, _ = os.path.splitext(chunks_csv_name)\n",
    "done_csv_name = f\"{basename}_done.csv\"\n",
    "\n",
    "# Lade die CSV mit den Chunked-Daten\n",
    "df_chunked = pd.read_csv(chunks_csv_name)\n",
    "\n",
    "# Erstelle die neue Spalte \"Top-3 alle\" mit dem gewünschten Satz\n",
    "df_chunked[\"Top-3 alle\"] = (\n",
    "    \"Die Top 3 Sachgebiete in diesem Text sind: \" +\n",
    "    df_chunked[\"Top-Sachgebiet 1\"].astype(str) + \", \" +\n",
    "    df_chunked[\"Top-Sachgebiet 2\"].astype(str) + \", \" +\n",
    "    df_chunked[\"Top-Sachgebiet 3\"].astype(str) + \".\"\n",
    ")\n",
    "\n",
    "# Spalten neu ordnen, um \"Top-3 alle\" zwischen \"Top-Sachgebiet 3\" und \"Text\" zu setzen\n",
    "cols = df_chunked.columns.tolist()\n",
    "\n",
    "# Finden des Index der Spalte \"Text\"\n",
    "insert_at = cols.index(\"Text\") if \"Text\" in cols else len(cols)\n",
    "\n",
    "# Entferne \"Top-3 alle\" von der letzten Stelle (falls es an irgendeiner Stelle zugefügt wurde)\n",
    "if \"Top-3 alle\" in cols:\n",
    "    cols.remove(\"Top-3 alle\")\n",
    "\n",
    "# Füge \"Top-3 alle\" an der richtigen Stelle zwischen \"Top-Sachgebiet 3\" und \"Text\" ein\n",
    "cols = cols[:insert_at] + [\"Top-3 alle\"] + cols[insert_at:]\n",
    "\n",
    "# Reorganisiere den DataFrame mit der neuen Spaltenreihenfolge\n",
    "df_chunked = df_chunked[cols]\n",
    "\n",
    "# Speichere das neue CSV\n",
    "df_chunked.to_csv(f\"{basename}_done.csv\", index=False)\n",
    "print(f\"✅ Done CSV '{done_csv_name}' wurde erfolgreich erstellt.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "7ea88515-328d-477e-8793-1c4ecaab9986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Dateiname          Top-Sachgebiet 1  \\\n",
      "0  162853_CP Liban_FR.txt  Struktur und Institution   \n",
      "1  162853_CP Liban_FR.txt  Struktur und Institution   \n",
      "\n",
      "                 Top-Sachgebiet 2  Top-Sachgebiet 3  \\\n",
      "0  Entwicklung und Zusammenarbeit  Humanitäre Hilfe   \n",
      "1  Entwicklung und Zusammenarbeit  Humanitäre Hilfe   \n",
      "\n",
      "                                          Top-3 alle  \\\n",
      "0  Die Top 3 Sachgebiete in diesem Text sind: Str...   \n",
      "1  Die Top 3 Sachgebiete in diesem Text sind: Str...   \n",
      "\n",
      "                                                Text  original_row_id  \\\n",
      "0  Communiqué de presse Berne 09. 12. 2024 ( TBC ...                0   \n",
      "1  ##ès équitable à l ’ eau potable pour tous les...                0   \n",
      "\n",
      "   chunk_id  \n",
      "0         1  \n",
      "1         2  \n",
      "(2, 8)\n",
      "['Dateiname', 'Top-Sachgebiet 1', 'Top-Sachgebiet 2', 'Top-Sachgebiet 3', 'Top-3 alle', 'Text', 'original_row_id', 'chunk_id']\n"
     ]
    }
   ],
   "source": [
    "# 4. Überprüfen des Ergebnisses\n",
    "basename, _ = os.path.splitext(chunks_csv_name)\n",
    "done_csv_name = f\"{basename}_done.csv\"\n",
    "\n",
    "# Lade das neue CSV\n",
    "df_new = pd.read_csv(done_csv_name)\n",
    "\n",
    "# Zeige die ersten 3 Zeilen für eine Vorschau\n",
    "print(df_new.head())\n",
    "\n",
    "# Zeige die Struktur der Datei\n",
    "print(df_new.shape)  # Zeilen und Spalten\n",
    "print(df_new.columns.tolist())  # Spaltennamen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "edb30247-d286-4a87-bec9-5fb17e0376d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Dateiname Top-Sachgebiet 1 Top-Sachgebiet 2  \\\n",
      "0  162799BFL_Stagiaire_FR.txt           Andere               HR   \n",
      "1  162799BFL_Stagiaire_FR.txt           Andere               HR   \n",
      "\n",
      "           Top-Sachgebiet 3  \\\n",
      "0  Struktur und Institution   \n",
      "1  Struktur und Institution   \n",
      "\n",
      "                                          Top-3 alle  \\\n",
      "0  Die Top 3 Sachgebiete in diesem Text sind: And...   \n",
      "1  Die Top 3 Sachgebiete in diesem Text sind: And...   \n",
      "\n",
      "                                                Text  original_row_id  \\\n",
      "0  Unnamed : 0 Unnamed : 1 Unnamed : 2 Unnamed : ...                0   \n",
      "1  ##plémentaire, veuillez - vous adresser à Mme ...                0   \n",
      "\n",
      "   chunk_id  \n",
      "0         1  \n",
      "1         2  \n"
     ]
    }
   ],
   "source": [
    "# 5. Filtern von Zeilen, um zu überprüfen, wie der Text aufgeteilt wurde\n",
    "\n",
    "# Filtern nach den ersten 3 unique original_row_ids\n",
    "first_three_ids = df_new[\"original_row_id\"].unique()[:3]\n",
    "\n",
    "# Zeilen mit diesen original_row_ids herausfiltern\n",
    "filtered_df = df_new[df_new[\"original_row_id\"].isin(first_three_ids)]\n",
    "\n",
    "# Anzeigen der gefilterten Zeilen\n",
    "print(filtered_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bc176ff4-4550-46ef-a7d0-81cba8afae99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Load Model and do Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "03e36365-8ff3-423f-943c-4ea27ae3c4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 2 Chunks geladen.\n",
      "🏷️  Topics-Kontext: Die Top 3 Sachgebiete in diesem Text sind: Struktur und Institution, Entwicklung und Zusammenarbeit, Humanitäre Hilfe.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d10db7996d448f49ab579950ece613a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📐 Embeddings erstellt.\n",
      "text_embedding.shape: (512,)\n",
      "topics_embedding.shape: (512,)\n",
      "query_embedding.shape: (1024,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# ---------- SCHRITT 1: CSV mit Text-Chunks + Topics laden ----------\n",
    "chunks_df = pd.read_csv(done_csv_name)\n",
    "chunks = chunks_df[\"Text\"].tolist()\n",
    "\n",
    "# Nehme die erste Zeile aus \"Top-3 alle\" (sie ist ja überall gleich)\n",
    "topics_text = chunks_df[\"Top-3 alle\"].iloc[0]\n",
    "print(f\"📚 {len(chunks)} Chunks geladen.\")\n",
    "print(f\"🏷️  Topics-Kontext: {topics_text}\")\n",
    "\n",
    "# ---------- SCHRITT 2: Modell laden & Chunks + Topics embeddieren ----------\n",
    "model = SentenceTransformer(\"sentence-transformers/distiluse-base-multilingual-cased-v2\")\n",
    "chunk_embeddings = model.encode(chunks, show_progress_bar=True)\n",
    "topics_embedding = model.encode(topics_text)\n",
    "print(\"📐 Embeddings erstellt.\")\n",
    "\n",
    "# ---------- SCHRITT 3: Durchschnitts-Vektor + Topics kombinieren (per Concatenate!) ----------\n",
    "text_embedding = np.mean(chunk_embeddings, axis=0)\n",
    "print(\"text_embedding.shape:\", text_embedding.shape)        # z.B. (512,)\n",
    "print(\"topics_embedding.shape:\", topics_embedding.shape)    # z.B. (512,)\n",
    "\n",
    "# Concatenate wie im Corpus!\n",
    "query_embedding = np.concatenate([text_embedding, topics_embedding])\n",
    "print(\"query_embedding.shape:\", query_embedding.shape)      # Sollte (1024,) sein\n",
    "\n",
    "# Jetzt kannst du wie gewohnt mit den corpus embeddings vergleichen, z.B.:\n",
    "# similarities = cosine_similarity([query_embedding], corpus_embeddings)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd9840f2-2980-496c-82a8-fab8972b24d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Load Database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "dd35cb3c-ee11-4e94-b2af-09e77a6655e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Embeddings geladen: Shape (6450, 1024)\n",
      "📝 Metadaten geladen: 6450 Einträge\n",
      "\n",
      "🔎 Vorschau der Metadaten:\n",
      "   original_row_id  chunk_id Translator  \\\n",
      "0                0         1        kas   \n",
      "1                0         2        kas   \n",
      "2                0         3        kas   \n",
      "\n",
      "                                          Top-3 alle  \n",
      "0  Die Top 3 Sachgebiete in diesem Text sind: Ent...  \n",
      "1  Die Top 3 Sachgebiete in diesem Text sind: Ent...  \n",
      "2  Die Top 3 Sachgebiete in diesem Text sind: Ent...  \n"
     ]
    }
   ],
   "source": [
    "# ---------- SCHRITT 4: Datenbank laden ----------\n",
    "embeddings = np.load(\"embeddings.npy\")\n",
    "metadata = pd.read_csv(\"metadata.csv\")\n",
    "\n",
    "# ✅ Zwischenausgabe: Prüfen, ob Embeddings & Metadaten korrekt geladen wurden\n",
    "print(f\"📂 Embeddings geladen: Shape {embeddings.shape}\")\n",
    "print(f\"📝 Metadaten geladen: {len(metadata)} Einträge\")\n",
    "\n",
    "# Zeige die ersten 3 Zeilen der Metadaten als Vorschau\n",
    "print(\"\\n🔎 Vorschau der Metadaten:\")\n",
    "print(metadata.head(3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08efc531-328b-4d9b-9b0c-c8334d2a0419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Give out Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "7b81b0ee-9d01-4102-b960-803e5d108e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Embedding Shape: (1024,)\n",
      "Corpus Embeddings Shape: (6450, 1024)\n"
     ]
    }
   ],
   "source": [
    "print(\"Query Embedding Shape:\", query_embedding.shape)  # Sollte (1024,) sein\n",
    "print(\"Corpus Embeddings Shape:\", embeddings.shape)     # Sollte (n, 1024) sein\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "f2673bdf-16ef-4228-b6ba-17657177a296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✨ Empfohlene Übersetzer:in: bec\n",
      "🔍 Ähnlichkeitswert: 0.7750152\n",
      "\n",
      "Top 3 Übersetzer:innen:\n",
      "1. bec (Ähnlichkeit: 0.7750)\n",
      "2. bec (Ähnlichkeit: 0.7562)\n",
      "3. bec (Ähnlichkeit: 0.7508)\n"
     ]
    }
   ],
   "source": [
    "# ---------- SCHRITT 5: Ähnlichste Übersetzer:innen finden ----------\n",
    "similarities = cosine_similarity([query_embedding], embeddings)[0]\n",
    "best_idx = similarities.argmax()\n",
    "beste_uebersetzerin = metadata.loc[best_idx, \"Translator\"]\n",
    "\n",
    "# ---------- AUSGABE ----------\n",
    "print(\"\\n✨ Empfohlene Übersetzer:in:\", beste_uebersetzerin)\n",
    "print(\"🔍 Ähnlichkeitswert:\", similarities[best_idx])\n",
    "\n",
    "# Optional: Top 3 anzeigen\n",
    "top_n = 3\n",
    "top_indices = similarities.argsort()[-top_n:][::-1]\n",
    "print(\"\\nTop 3 Übersetzer:innen:\")\n",
    "for rank, idx in enumerate(top_indices, 1):\n",
    "    print(f\"{rank}. {metadata.loc[idx, 'Translator']} (Ähnlichkeit: {similarities[idx]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb4db1a-8528-41ba-a729-12bb10092843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gewichtung anpassen: \n",
    "# query_embedding = 0.7 * text_embedding + 0.3 * topics_embedding\n",
    "# ➡️ Beispiel: Wenn du willst, dass die Topics stärker zählen, erhöhe z. B. auf 0.5 * text_embedding + 0.5 * topics_embedding."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
